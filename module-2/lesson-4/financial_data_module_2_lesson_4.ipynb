{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmtwRxgDCdVH"
   },
   "source": [
    "## FINANCIAL DATA\n",
    "MODULE 2 | LESSON 4\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jMxT11OCdVJ"
   },
   "source": [
    "# USING STATISTICAL DISTRIBUTIONS TO MODEL STOCK RETURNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGWZyTTGCdVK"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** |  30 minutes |\n",
    "|**Prior Knowledge** | Statistics, Cumulative distribution functions |\n",
    "|**Keywords** | Normal Distribution, Student T distribution, Normality Testing, P-values |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKVQbX9NCdVK"
   },
   "source": [
    "*We looked at various ways to measure risk and returns in the last lesson. Here, we take that a step further by using distributions to correctly model risk and returns.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1698950382391,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "AU_AEv3xCdVK"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import seaborn as sns\n",
    "import yfinance as yfin\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "yfin.pdr_override()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1fe1KODCdVL"
   },
   "source": [
    "## 1. How Are Stock Returns Distributed?\n",
    "Many models and theories surrounding stocks assume a normal distribution. We will try to determine that here with a data-based analysis. Properties of a Gaussian distribution are as follows:\n",
    "\n",
    "* Mean, median, and mode are all the same\n",
    "* The data is symmetrical, meaning there are equal counts of observations on both sides of the mean.\n",
    "* In normally distributed data, 68.25% of all cases fall within +/- one standard deviation from the mean, 95% of all cases fall within +/- two standard deviations from the mean, and 99.7% of all cases fall within +/- three standard deviations from the mean.\n",
    "\n",
    "Let's start by pulling 20 years of daily price data for the S&P 500. We'll use similar methods we've used in the last few lessons to pull this data and will calculate the log returns here.\n",
    "\n",
    "One quick way of doing this is to determine how many data points we have on either side of the mean here. We have a bit more than 5,000 data points. The below code takes the count of data points greater than the mean and divides it by the total number of data points. This will give us the percentage of data points greater than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1698950460511,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "PBK3uHHUCdVM"
   },
   "outputs": [],
   "source": [
    "# start = datetime.date.today() - datetime.timedelta(365 * 20)\n",
    "# end = datetime.date.today()\n",
    "end = datetime.date(2021, 11, 20)\n",
    "start = end - datetime.timedelta(365 * 20)\n",
    "\n",
    "prices = pd.DataFrame(web.DataReader([\"^GSPC\"], start, end)[\"Adj Close\"])\n",
    "\n",
    "# Rename column to make names more intuitive\n",
    "prices = prices.rename(columns={\"Adj Close\": \"SP500\"})\n",
    "df = np.log(prices) - np.log(prices.shift(1))\n",
    "df = df.iloc[1:, 0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsR1kEpiCdVM"
   },
   "source": [
    "### 1.1 Are returns symmetric?\n",
    "\n",
    "One quick way of doing this is to determine how many data points we have on either side of the mean here. We have a bit more then 5,000 data points here. The below code takes the count of data points greater than the mean and divides it by the total data points. This will give us the percentage of data points greater than the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1698950462328,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "LHDtF5B5CdVN"
   },
   "outputs": [],
   "source": [
    "(len(df[df.SP500 > df.SP500.mean()])) / (len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVBwz-geCdVN"
   },
   "source": [
    "We're getting about 52.6% of data points being greater than the mean, which shows we have a slightly negative skew to this dataset. We can't rule out symmetric returns based on this since it is only a sample of data and is reasonably close to the 50% mark. This makes it hard to say for certain whether S&P 500 returns are symmetric or not, but it is still a reasonable assumption to make here. Also, keep in mind there should be a slight bias towards positive returns anyways, if for no other reason than some drift from inflation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG-3rnkiCdVN"
   },
   "source": [
    "### 1.2 Is Volatility constant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "executionInfo": {
     "elapsed": 792,
     "status": "ok",
     "timestamp": 1698950467177,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "mQ5dkm2ICdVN"
   },
   "outputs": [],
   "source": [
    "vols = pd.DataFrame(df.SP500.rolling(50).std()).rename(columns={\"SP500\": \"S&P 500 STD\"})\n",
    "\n",
    "# set figure size\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# plot using rolling average\n",
    "sns.lineplot(\n",
    "    x=\"Date\",\n",
    "    y=\"S&P 500 STD\",\n",
    "    data=vols,\n",
    "    label=\"S&P 500 50 day standard deviation rolling avg\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8nkQT-lCdVO"
   },
   "source": [
    "Above, we calculate the rolling standard deviation using a window of 50 days and then make a line chart of it. It can be clearly seen that volatility is anything but constant. This adds another layer of complexity to modeling stock returns, especially the many models which assume constant volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FmXRxmtCdVO"
   },
   "source": [
    "## 2. Are Stock Returns Normally Distributed?\n",
    "The normal distribution is one of the most common distributions used in modeling random variables. Indeed, many phenomena in the natural and social sciences can be modeled by normal distribution. One of the great advantages of the normal distribution is its simplicity. We can completely describe a normal distribution through two numbers: one for the center of the distribution and one for the uncertainty about that center. The first number refers to the mean, and the second number refers to the standard deviation.\n",
    "\n",
    "Once we have these two numbers, we can draw inferences, estimate percentiles, compute probabilities that a point falls within a region, and more. If our data is well-represented by the normal distribution, then we can confidently use the mean and standard deviation to report our portfolio expected returns and volatilities. If our data is not well represented by the normal distribution, then we need to find other distributions that are more suitable. Thus, when we have a distribution of stock returns, for example, we'll want to start this assessment by visualizing the returns to see if they appear to be normal. Of course, we can follow this up with more quantitative assessments by running statistical tests.\n",
    "\n",
    "We can visualize the data using the hist() method. We pass in bins = 100 as a parameter to determine the amount of buckets to place the data in. The more bins you have, the more granular the data will look in a histogram. Increasing the bins too much may result in slightly noisy data, which will make it tougher to determine a normal distribution. The chart in Figure 3 looks like it could be normally distributed, but we need to be a little more scientific to determine if that's actually the case or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1698950472588,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "kdoRxLomCdVP"
   },
   "outputs": [],
   "source": [
    "df.hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDVBfbSZCdVP"
   },
   "source": [
    "### 2.1 Conducting a normality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1698950476926,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "h0c6JIw9CdVP"
   },
   "outputs": [],
   "source": [
    "stats.normaltest((np.array(df.SP500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NetxcwY8CdVP"
   },
   "source": [
    "We can use the `normaltest()` method here to determine if the sample data could fit a normal distribution. This method uses D'Agostino and Pearson's normality test, which combines skew and kurtosis to produce an omnibus test of normality.\n",
    "\n",
    "The null hypothesis of this test is that the sample data fits a normal distribution. Let's assume we want to be 90% confident this data fits a normal distribution. We can compare this to the p-value to see if it's greater than 90%. In this case, the value, 3.61e-252, is extremely small, which leads us to reject the null hypothesis that this data fits a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omnZwIfhCdVQ"
   },
   "source": [
    "### 2.2 Testing Skewness and Kurtosis\n",
    "As one added testing step, we can test the skewness and kurtosis of our distribution using the Jarque-Bera test. The test statistic will always be greater than zero. The further the test statistic is from zero, the more likely the sample data does not match a normal distribution.\n",
    "\n",
    "Lucky for us, Python has another library for us to use here which really simplifies the analysis. From the `scipy.stats` library, we can use the `jarque_bera()` method directly to our data to get test statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1698950507266,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "AHFPOCMgCdVQ"
   },
   "outputs": [],
   "source": [
    "stats.jarque_bera((np.array(df.SP500))).pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPwMW_bVCdVQ"
   },
   "source": [
    "The p value of our data here shows once and for all that there is virtually zero chance our sample data is normally distributed<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrWyj_SJCdVQ"
   },
   "source": [
    "### 2.3 Where Does Our Gaussian Distribution Break Down?\n",
    "So according to the normality test, our data is not normally distributed despite the histogram looking like it may be. So, why is the data failing the normality test? The answer likely comes down to fat tails. Fat tails essentially means that extreme events +/-3 standard deviations away from the mean) are more likely than the normal distribution would imply.\n",
    "\n",
    "Assuming a normal distribution with a mean of 0.00028 and standard deviation of 0.012, we can determine the probability of any return given that the returns fit a normal distribution.\n",
    "\n",
    "To determine how many standard deviations away from the mean a specific number is, we need to use\n",
    "\n",
    "$$\\frac{X - \\bar{X}}{\\text{Sample standard deviation}}$$\n",
    "\n",
    " Let's do this for the min and max of the sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1698950515245,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "yxg3mlnICdVQ"
   },
   "outputs": [],
   "source": [
    "dfMax = df.SP500.max()\n",
    "dfMin = df.SP500.min()\n",
    "print(\n",
    "    \"Min return of sample data is %.4f and the maximum return of sample data is %.4f\"\n",
    "    % (dfMin, dfMax)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1698950519568,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "D4JflG2zCdVR"
   },
   "outputs": [],
   "source": [
    "df.SP500.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1698950523788,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "WoKK6f5DCdVR"
   },
   "outputs": [],
   "source": [
    "df.SP500.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1698950526780,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "zyWxZ2NkCdVR"
   },
   "outputs": [],
   "source": [
    "(df.SP500.min() - df.SP500.mean()) / df.SP500.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1698950528636,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "O6L1vN56CdVR"
   },
   "outputs": [],
   "source": [
    "(df.SP500.max() - df.SP500.mean()) / df.SP500.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr1ysBRsCdVS"
   },
   "source": [
    "Over the last 20 years, S&P 500 has had a maximum daily return of 10.96% and a minimum daily return of -12.77%. If we use the formula to determine standard deviations from the mean, we get -10.5 and 8.9 standard deviations away from the mean for the minimum and maximum, respectively. These standard deviations are humongous when compared to the normal distribution. We can see this analytically when we plug in the z score to the `norm.cdf()` method to determine the probability this value could be in a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1698950543261,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "aUlblqLkCdVS"
   },
   "outputs": [],
   "source": [
    "stats.norm.cdf(-10.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCdiUT_VCdVS"
   },
   "source": [
    "This implies that the chance we could have a move as small as -12.77%, is 7.326261431744285e-26. This probability is so low that we would never expect an event like this to happen in our lifetime. We have multiple events like this, as illustrated by the minimum and maximum.\n",
    "\n",
    "Going further with this idea, based on normal distribution z tables, we would expect 99.7% of our data points to be within +/- 3 standard deviations from the mean. Let's determine this for our sample data. First off, we need to find the cut-off values at +/- 3 standard deviations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1698950565298,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "uzhMHMrVCdVS"
   },
   "outputs": [],
   "source": [
    "(3 * df.SP500.std()) + df.SP500.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1698950567072,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "BHYU9bYyCdVT"
   },
   "outputs": [],
   "source": [
    "(-3 * df.SP500.std()) + df.SP500.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hPB7TBzCdVU"
   },
   "source": [
    "The above two calculations would imply that 99.7% of all of our data points should be in between -0.0364 and 0.03699. Since we have 5,031 data points, we would expect about 15 of them to be outside of that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1698950589257,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "EpI05MTCCdVU"
   },
   "outputs": [],
   "source": [
    "df[(df[\"SP500\"] > 0.03699) | (df[\"SP500\"] < -0.0364)].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1698950594400,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "_yexL4Q6CdVU"
   },
   "outputs": [],
   "source": [
    "len(df[(df[\"SP500\"] > 0.05) | (df[\"SP500\"] < -0.05)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrxmtgwRCdVU"
   },
   "source": [
    "Not only do we get 15 values outside of our 3 standard deviation range, but we also get 36 values outside of +/- 5%, though you would almost never expect one of these events over 20 years, given a normal distribution.\n",
    "\n",
    "In this next video, we go over how to leverage Python in order to test for normality of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1697622068299,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -180
    },
    "id": "YDDl-6-zCdVU"
   },
   "outputs": [],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"706653140\", h=\"47eb01d16b\", width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRTcl1X7CdVV"
   },
   "source": [
    "##### [Access video transcript here](https://drive.google.com/file/d/1aJ1WcWvEEM5cUnJZm4lDdyOug1j2E9Rs/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A4qFnBkCdVV"
   },
   "source": [
    "All this analysis does is basically prove that we have fat tails in our sample data, which is why the normal distribution is not suitable for modeling stock returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95C8VuEZCdVV"
   },
   "source": [
    "## 3. Non-Gaussian Distributions\n",
    "\n",
    "One potential alternative distribution we could use to forecast stock returns is the Student's t-distribution. This is very similar to a normal distribution except it has heavier tails. Theoretically, this sounds perfect for daily returns based on what we've seen up to this point.\n",
    "\n",
    "Lets proceed with a visual inspection of the distribution of our data by superimposing the normal distribution on the KDE of S&P500 returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1698950991249,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "n0ZeuLfyRvw0"
   },
   "outputs": [],
   "source": [
    "# Sampling from normal distribution\n",
    "np.random.seed(222)\n",
    "normal_dist = stats.norm.rvs(\n",
    "    size=len(df[\"SP500\"]), loc=df[\"SP500\"].mean(), scale=df[\"SP500\"].std()\n",
    ")\n",
    "\n",
    "# Creating an additional column in df in order to use the KDE plot functionality of pandas\n",
    "df[\"Normal Sample\"] = normal_dist\n",
    "\n",
    "# Plotting the KDE plots\n",
    "df[[\"SP500\", \"Normal Sample\"]].plot(kind=\"kde\", xlim=(-0.1, 0.1), figsize=(12, 4))\n",
    "\n",
    "\"\"\"#Using Seaborn to create KDE\n",
    "plt.figure(figsize = (12,4))\n",
    "kde = sns.kdeplot(df, fill=True, alpha=.5, linewidth=0).set_xlim(-0.1, 0.1);\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tge68c0leGMs"
   },
   "source": [
    "The SP500 returns seem a lot more leptokurtic. Indeed the excess kurtosis of SP500 is greater than 0:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1698950995497,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "STPDMEBCebIq"
   },
   "outputs": [],
   "source": [
    "df.SP500.kurt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RUgruN2ejTb"
   },
   "source": [
    "The tails of SP500 are also fatter than those of a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1698950999333,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "S3zgE6yjeieA"
   },
   "outputs": [],
   "source": [
    "# Observing the tails\n",
    "df[[\"SP500\", \"Normal Sample\"]].plot(\n",
    "    kind=\"kde\", xlim=(-0.075, -0.04), ylim=(-1, 2), figsize=(12, 4)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0YdA0zPf0Tn"
   },
   "source": [
    "At this stage, we will calibrate the parameters of the Student's t-distribution using Maximum Likelihood Estimation (MLE) to align the distribution closely with our observed data.\n",
    "\n",
    "From the next visual inspection, it's evident that the synthetic data generated from the fitted Student's t-distribution offers a more accurate approximation of our actual data compared to what we got from a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1698951003696,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "NkVFYbGWSOXc"
   },
   "outputs": [],
   "source": [
    "# Fit the t-distribution using MLE: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.fit.html\n",
    "params = stats.t.fit(df.SP500)\n",
    "\n",
    "# We plot the fitted distribution against the kde of the data\n",
    "df[\"t-Sample *Fitted*\"] = stats.t.rvs(*params, size=len(df))\n",
    "df[[\"SP500\", \"t-Sample *Fitted*\"]].plot(kind=\"kde\", figsize=(12, 4), xlim=(-0.1, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv8Z-nFEh5Xp"
   },
   "source": [
    "The tails also seem to be better explained by the t-distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1698951010876,
     "user": {
      "displayName": "Tea Toradze",
      "userId": "13683888799925082799"
     },
     "user_tz": 0
    },
    "id": "VyUJWMmwgTeL"
   },
   "outputs": [],
   "source": [
    "df[[\"SP500\", \"t-Sample *Fitted*\"]].plot(\n",
    "    kind=\"kde\", figsize=(12, 4), xlim=(-0.075, -0.04), ylim=(-1, 2)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq1PSgbCCdVX"
   },
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "In this lesson, we focused for the first time on comparing daily stock return data to normal distributions. We also touched on other potential distributions we could use to model this data. In the next module, we will take what we've learned thus far and apply it to a portfolio setting instead of just looking at individual assets in isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqF9v1OvCdVX"
   },
   "source": [
    "**References**\n",
    "\n",
    "- D'Agostino, Ralph B. \"An Omnibus Test of Normality for Moderate and Large Size Samples.\" *Biometrika*, vol. 58, no. 2, 1971, pp. 341–348, https://doi.org/10.1093/biomet/58.2.341."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvXoOU2HCdVX"
   },
   "source": [
    "---\n",
    "Copyright 2023 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
